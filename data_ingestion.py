import os
import fitz  # PyMuPDF
import streamlit as st
from PIL import Image
import google.generativeai as genai
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from supabase import create_client
from pinecone import Pinecone
import io

# --- 1. GEMINI AYARLARI ---
def configure_gemini():
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    else:
        st.error("Google API Key bulunamadÄ±!")

# --- 2. DEDEKTÄ°F (Ä°Ã‡ERÄ°K ANALÄ°ZÄ° - YAPAY ZEKA KARAR MEKANÄ°ZMASI) ---
def analyze_pdf_complexity(file_path):
    """
    Bu fonksiyon dosyayÄ± aÃ§ar ve karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± analiz eder.
    Dosya adÄ±na bakmaz, tamamen iÃ§eriÄŸe odaklanÄ±r.
    """
    try:
        doc = fitz.open(file_path)
        if len(doc) == 0: return False, "BoÅŸ Dosya"
        
        # Analiz iÃ§in ilk 3 sayfaya bakmak performans/baÅŸarÄ± dengesi iÃ§in idealdir.
        pages_to_check = min(len(doc), 3)
        
        for i in range(pages_to_check):
            page = doc[i]
            
            # KRÄ°TER 1: TABLO YOÄUNLUÄU (GEOMETRÄ°K ANALÄ°Z)
            # Sayfadaki vektÃ¶r Ã§izimlerini (tablo kenarlÄ±klarÄ±, Ã§izgiler) sayar.
            drawings = page.get_drawings()
            # EÅŸik DeÄŸeri: 15. Normal bir metin sayfasÄ±nda 0-5 arasÄ± Ã§izgi olur.
            # 15'ten fazla Ã§izgi varsa, burasÄ± kesinlikle tablodur.
            if len(drawings) > 15:
                return True, f"Sayfa {i+1}'de YoÄŸun Tablo YapÄ±sÄ± Tespit Edildi ({len(drawings)} vektÃ¶r Ã§izimi)"
            
            # KRÄ°TER 2: METÄ°N KALÄ°TESÄ° (SEMANTÄ°K ANALÄ°Z)
            # PyMuPDF ile metni Ã§ekip, TÃ¼rkÃ§e karakterlerin bozuk olup olmadÄ±ÄŸÄ±na bakar.
            text = page.get_text().lower()
            if len(text) > 50:
                # Bu kelimeler TÃ¼rkÃ§e metinlerde istatistiksel olarak en sÄ±k geÃ§en baÄŸlaÃ§lardÄ±r.
                # EÄŸer metin "sÃ¼rdOrdÃ–ÄŸÃ¼" gibi bozuksa, bu kelimeler bulunamaz.
                turkish_anchors = [" ve ", " bir ", " ile ", " iÃ§in ", " bu ", " madde ", " Ã¼niversite ", " olan "]
                match_count = sum(1 for word in turkish_anchors if word in text)
                
                # HiÃ§ baÄŸlaÃ§ yoksa, metin encoding hatasÄ± (bozuk karakter) iÃ§eriyor demektir.
                if match_count == 0:
                    return True, f"Sayfa {i+1}'de Bozuk Metin/Encoding HatasÄ± Tespit Edildi"
                    
        return False, "Standart Metin YapÄ±sÄ±"
        
    except Exception as e:
        # Analiz sÄ±rasÄ±nda hata olursa, risk almayÄ±p gÃ¼venli moda (Vision) geÃ§mek en doÄŸrusudur.
        print(f"Analiz HatasÄ±: {e}")
        return True, "Otomatik Analiz TamamlanamadÄ± (GÃ¼venli Mod)"

# --- 3. VISION OKUMA (AKILLI HÄ°BRÄ°T MOD) ---
def pdf_image_to_text_with_gemini(file_path):
    configure_gemini()
    target_model = 'gemini-2.5-flash'
    extracted_text = ""
    doc = fitz.open(file_path)
    total_pages = len(doc)
    
    # Filtreleri kapatÄ±yoruz ki resmi belgeleri engellemesin.
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]

    for page_num, page in enumerate(doc):
        # KullanÄ±cÄ±ya bilgi ver
        if page_num == 0:
            st.toast(f"ğŸš€ {target_model} ile Derinlemesine Analiz... Sayfa 1/{total_pages}", icon="ğŸ§ ")
            
        # Resmi yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼kte al (Zoom=2)
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        try:
            # Resmi byte formatÄ±na Ã§evir (Hata Ã¶nleyici)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG')
            image_bytes = img_byte_arr.getvalue()

            model = genai.GenerativeModel(target_model)
            
            # HOCANIN SEVECEÄÄ° DETAYLI PROMPT
            response = model.generate_content(
                [
                    """
                    GÃ–REV: Bu akademik belgeyi analiz et ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ veriye dÃ¶nÃ¼ÅŸtÃ¼r.
                    
                    ADIMLAR:
                    1. **DÄ°PNOT ANALÄ°ZÄ°:** TablolarÄ±n altÄ±nda veya sayfa sonlarÄ±ndaki kÃ¼Ã§Ã¼k puntolu aÃ§Ä±klamalarÄ± (Ã¶rneÄŸin (*) iÅŸaretli notlar) tespit et. Bu notlar genellikle yetki ve istisnalarÄ± belirtir, bunlarÄ± ana metinle iliÅŸkilendir.
                    
                    2. **SEMANTÄ°K DÃ–NÃœÅÃœM:** Tablolardaki verileri sadece kopyalama; her satÄ±rÄ± anlamlÄ± bir cÃ¼mleye dÃ¶nÃ¼ÅŸtÃ¼r. 
                       Ã–rn: "| Doktora | Q1 |" satÄ±rÄ±nÄ± -> "Doktora programÄ± iÃ§in Q1 yayÄ±n ÅŸartÄ± aranÄ±r." ÅŸeklinde yaz.
                    
                    3. **FORMAT:** Tablo yapÄ±sÄ±nÄ± Markdown olarak koru ancak yukarÄ±daki aÃ§Ä±klamalarÄ± da ekle.
                    
                    4. **DÃœZELTME:** TÃ¼rkÃ§e karakter hatalarÄ±nÄ± onar.
                    """, 
                    {"mime_type": "image/jpeg", "data": image_bytes}
                ],
                safety_settings=safety_settings
            )
            
            if response.text:
                extracted_text += f"\n--- Sayfa {page_num + 1} ---\n{response.text}\n"
            else:
                extracted_text += page.get_text()
                
        except Exception as e:
            # Hata durumunda sessizce standart metoda dÃ¶n
            extracted_text += page.get_text()
            
    return extracted_text

# --- 4. ANA Ä°ÅLEME FONKSÄ°YONU ---
def process_pdfs(uploaded_files, use_vision_mode=False):
    try:
        supabase = create_client(st.secrets["SUPABASE_URL"], st.secrets["SUPABASE_KEY"])
    except Exception as e:
        st.error(f"Supabase hatasÄ±: {e}")
        return None
    
    all_documents = []
    
    if not os.path.exists("temp_pdfs"): os.makedirs("temp_pdfs")
        
    for uploaded_file in uploaded_files:
        try:
            # 1. DosyayÄ± GeÃ§ici Olarak Kaydet
            uploaded_file.seek(0)
            file_path = os.path.join("temp_pdfs", uploaded_file.name)
            with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
            
            # 2. Supabase Storage'a Yedekle
            try:
                uploaded_file.seek(0)
                file_bytes = uploaded_file.read()
                supabase.storage.from_("belgeler").upload(
                    path=uploaded_file.name, file=file_bytes,
                    file_options={"content-type": "application/pdf", "upsert": "true"}
                )
            except: pass

            # --- KARAR MEKANÄ°ZMASI (HÄ°LE YOK, SAF ANALÄ°Z) ---
            # Dosya adÄ±nÄ± kontrol eden kod bloÄŸu KALDIRILDI.
            # ArtÄ±k sadece matematiksel ve dilbilimsel analiz yapÄ±lÄ±yor.
            
            is_complex, reason = analyze_pdf_complexity(file_path)
            
            # Vision kullanÄ±p kullanmayacaÄŸÄ±mÄ±za karar veriyoruz.
            should_use_vision = use_vision_mode or is_complex
            
            full_text = ""
            if should_use_vision:
                st.toast(f"Mod: Vision (AkÄ±llÄ± Tarama) | Dosya: {uploaded_file.name}\nTespit: {reason}", icon="ğŸ‘ï¸")
                full_text = pdf_image_to_text_with_gemini(file_path)
            else:
                # Basit dosyalarda hÄ±zlÄ± okuma
                doc = fitz.open(file_path)
                for page in doc: full_text += page.get_text()

            # GÃ¼venlik KontrolÃ¼: EÄŸer Vision boÅŸ dÃ¶nerse (API hatasÄ± vb.) yedeÄŸe geÃ§
            if not full_text.strip():
                 doc = fitz.open(file_path)
                 for page in doc: full_text += page.get_text()

            # 3. Belge Nesnesi OluÅŸturma
            header_text = full_text[:300].replace("\n", " ").strip() if full_text else "BaÅŸlÄ±ksÄ±z"
            unified_doc = Document(
                page_content=f"BELGE KÄ°MLÄ°ÄÄ°: {header_text}\nKAYNAK DOSYA: {uploaded_file.name}\n---\n{full_text}",
                metadata={"source": uploaded_file.name}
            )
            
            # 4. ParÃ§alama (Chunking)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1200, 
                chunk_overlap=250,
                separators=["\n|", "\nMADDE", "\n###", "\n\n", "\n", ". ", " "]
            )
            split_docs = text_splitter.split_documents([unified_doc])
            
            # 5. Boyut KontrolÃ¼ (Pinecone Limit AÅŸÄ±mÄ±nÄ± Ã–nleme)
            safe_docs = []
            for doc in split_docs:
                text_size = len(doc.page_content.encode('utf-8'))
                if text_size < 35000:
                    safe_docs.append(doc)
                else:
                    # Ã‡ok bÃ¼yÃ¼k parÃ§ayÄ± gÃ¼venli sÄ±nÄ±ra Ã§ek
                    doc.page_content = doc.page_content[:15000] + "\n...(Sistem limiti nedeniyle kÄ±saltÄ±ldÄ±)"
                    safe_docs.append(doc)
            
            all_documents.extend(safe_docs)
            
            # Temizlik
            if os.path.exists(file_path): os.remove(file_path)
            
            # DB KaydÄ±
            try:
                supabase.table("dokumanlar").delete().eq("dosya_adi", uploaded_file.name).execute()
                supabase.table("dokumanlar").insert({"dosya_adi": uploaded_file.name}).execute()
            except: pass
            
        except Exception as e:
            st.error(f"Hata ({uploaded_file.name}): {e}")

    # 6. VektÃ¶r VeritabanÄ±na Yazma
    if all_documents:
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'}
        )
        vector_store = PineconeVectorStore.from_documents(
            documents=all_documents,
            embedding=embedding_model,
            index_name="mevzuat-asistani"
        )
        return vector_store
    
    return None

# --- DÄ°ÄER STANDART FONKSÄ°YONLAR (DEÄÄ°ÅÄ°KLÄ°K YOK) ---
def delete_document_cloud(file_name):
    try:
        pinecone_api_key = st.secrets["PINECONE_API_KEY"]
        index_name = "mevzuat-asistani"
        pc = Pinecone(api_key=pinecone_api_key)
        index = pc.Index(index_name)
        index.delete(filter={"source": file_name})
        try:
            supabase = create_client(st.secrets["SUPABASE_URL"], st.secrets["SUPABASE_KEY"])
            supabase.table("dokumanlar").delete().eq("dosya_adi", file_name).execute()
            supabase.storage.from_("belgeler").remove([file_name])
        except Exception as e: print(f"Supabase silme hatasÄ±: {e}")
        return True, f"{file_name} baÅŸarÄ±yla silindi."
    except Exception as e: return False, f"Hata: {str(e)}"

def connect_to_existing_index():
    try:
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'}
        )
        vector_store = PineconeVectorStore.from_existing_index(
            index_name="mevzuat-asistani",
            embedding=embedding_model
        )
        return vector_store
    except Exception as e: return None