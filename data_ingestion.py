import time
import os
import fitz  # PyMuPDF
import streamlit as st
from PIL import Image
import google.generativeai as genai
from langchain_pinecone import PineconeVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from supabase import create_client
from pinecone import Pinecone
import io
import collections

# --- 1. GEMINI AYARLARI ---
def configure_gemini():
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    else:
        st.error("Google API Key bulunamadÄ±!")

# --- 2. SÃœTUN HÄ°ZALAMA ANALÄ°ZÄ°  ---
def analyze_pdf_complexity(file_path):
    """
    Belgedeki metinlerin sol hizalamasÄ±na (X koordinatÄ±na) bakar.
    YÃ¶netmelik girintilerini (indentation) tablo sÃ¼tunu sanmamasÄ± iÃ§in
    daha akÄ±llÄ± bir yoÄŸunluk kontrolÃ¼ yapar..
    """
    try:
        doc = fitz.open(file_path)
        if len(doc) == 0: return False, "BoÅŸ Dosya"
        
        # Ä°lk 3 sayfayÄ± tara
        pages_to_check = min(len(doc), 3)
        
        for i in range(pages_to_check):
            page = doc[i]
            
            # Kelimelerin koordinatlarÄ±nÄ± al
            text_dict = page.get_text("dict")
            x_starts = []
            
            for block in text_dict["blocks"]:
                if "lines" in block:
                    for line in block["lines"]:
                        for span in line["spans"]:
                            # Ã‡ok kÄ±sa yazÄ±larÄ± (Madde no, a), b) gibi) ve boÅŸluklarÄ± atla.
                            # Ã‡Ã¼nkÃ¼ bunlar "SÃ¼tun" deÄŸil, "Madde Ä°ÅŸaretidir".
                            if len(span["text"].strip()) > 5:
                                x_starts.append(round(span["bbox"][0] / 20) * 20)
            
            # EÄŸer sayfada hiÃ§ anlamlÄ± yazÄ± yoksa (TaranmÄ±ÅŸ PDF), direkt Vision.
            if not x_starts:
                return True, "Metin BulunamadÄ± (Resim PDF)"

            # --- ANALÄ°Z ---
            # X koordinatlarÄ±nÄ±n frekansÄ±nÄ± say.
            counter = collections.Counter(x_starts)
            
            # En sÄ±k tekrar eden hizalamalarÄ± al
            most_common_alignments = counter.most_common()
            
            # EÅŸik DeÄŸer: GerÃ§ek bir sÃ¼tun olmasÄ± iÃ§in o hizada EN AZ 15 SATIR olmalÄ±.
            # YÃ¶netmelikteki a) b) c) ÅŸÄ±klarÄ± genelde 3-5 satÄ±r sÃ¼rer, bu yÃ¼zden elenirler.
            # Tablolar ise sayfa boyu sÃ¼rdÃ¼ÄŸÃ¼ iÃ§in 20-30 satÄ±r olur.
            significant_columns = 0
            active_columns = [] # Debug iÃ§in
            
            for x_pos, count in most_common_alignments:
                if count >= 15: # KRÄ°TÄ°K EÅÄ°K: 15 SatÄ±r
                    significant_columns += 1
                    active_columns.append(f"X={x_pos} ({count} satÄ±r)")
            
            # KARAR: 
            # 3 veya daha fazla "YOÄUN" sÃ¼tun varsa VISION AÃ‡.
            # (YÃ¶netmeliklerde genelde sadece 1 yoÄŸun sÃ¼tun olur: Ana Metin)
            if significant_columns >= 3:
                return True, f"Ã‡oklu SÃ¼tun YapÄ±sÄ± Tespit Edildi ({significant_columns} sÃ¼tun: {active_columns})"
                
            # --- YEDEK KELÄ°ME KONTROLÃœ  ---
            text_plain = page.get_text().lower()
            # Sadece 'Q1' ve 'Ã‡eyreklik' kelimeleri bir aradaysa aÃ§ (Tez Tablosu iÃ§in sigorta)
            if "q1" in text_plain and "Ã§eyreklik" in text_plain:
                return True, "Akademik Terim (Q1) Tespit Edildi"

        return False, "Standart AkÄ±ÅŸ Metni"
        
    except Exception as e:
        print(f"Analiz HatasÄ±: {e}")
        return False, "Analiz HatasÄ± -> Standart Mod"

# --- 3. VISION OKUMA (SESSÄ°Z VE GÃœVENLÄ°) ---
def pdf_image_to_text_with_gemini(file_path):
    configure_gemini()
    target_model = 'gemini-2.5-flash'
    extracted_text = ""
    doc = fitz.open(file_path)
    
    st.toast(f"ğŸ‘ï¸ VISION MODU: {os.path.basename(file_path)}", icon="ğŸ“¸")
    
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]

    for page_num, page in enumerate(doc):
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        try:
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='JPEG')
            image_bytes = img_byte_arr.getvalue()

            model = genai.GenerativeModel(target_model)
            
            prompt = """
            GÃ–REV: Bu akademik belgeyi analiz et.
            1. EÄŸer sayfada TABLO varsa, tabloyu bozmadan Markdown formatÄ±na Ã§evir.
            2. Tablodaki her satÄ±rÄ±n baÅŸÄ±na, o satÄ±rÄ±n ait olduÄŸu ana baÅŸlÄ±ÄŸÄ± (Ã–rn: "DOKTORA") ekle.
            3. **KRÄ°TÄ°K - TABLO ALTI NOTLAR:** Tablonun hemen altÄ±nda veya sayfanÄ±n en altÄ±nda yer alan cÃ¼mlelere DÄ°KKAT ET.
               - Ã–zellikle **"...karar verir"**, **"...yetkilidir"**, **"...Kurulu"** gibi ifadeler iÃ§eren cÃ¼mleleri ASLA ATLAMA.
               - Bu cÃ¼mleleri **"GENEL HÃœKÃœM: [CÃ¼mle]"** formatÄ±nda metnin en baÅŸÄ±na ekle.
               
            """
            
            response = model.generate_content(
                [prompt, {"mime_type": "image/jpeg", "data": image_bytes}],
                safety_settings=safety_settings
            )
            
            try:
                if hasattr(response, 'text') and response.text:
                    extracted_text += f"\n--- Sayfa {page_num + 1} ---\n{response.text}\n"
                else:
                    raise ValueError("BoÅŸ Cevap")
            except Exception:
                # Sessizce yedeÄŸe geÃ§
                print(f"Sayfa {page_num+1} Vision okuyamadÄ±, standart moda geÃ§ildi.")
                extracted_text += page.get_text()

        except Exception as e:
            print(f"API HatasÄ±: {e}")
            extracted_text += page.get_text()
            
    return extracted_text

# --- 4. ANA Ä°ÅLEME FONKSÄ°YONU ---

def process_pdfs(uploaded_files, use_vision_mode=False):
    try:
        supabase = create_client(st.secrets["SUPABASE_URL"], st.secrets["SUPABASE_KEY"])
    except Exception as e:
        st.error(f"Supabase hatasÄ±: {e}")
        return None
    
    all_documents = []
    
    if not os.path.exists("temp_pdfs"): os.makedirs("temp_pdfs")
        
    for uploaded_file in uploaded_files:
        try:
            uploaded_file.seek(0)
            file_path = os.path.join("temp_pdfs", uploaded_file.name)
            with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
            
            # --- DEDEKTÄ°F KARARI ---
            is_complex, reason = analyze_pdf_complexity(file_path)
            
            if is_complex:
                st.warning(f"ğŸŸ  Vision Modu: {uploaded_file.name}\nSebep: {reason}")
            else:
                st.success(f"ğŸŸ¢ HÄ±zlÄ± Mod: {uploaded_file.name}\nSebep: {reason}")
            
            should_use_vision = use_vision_mode or is_complex
            
            full_text = ""
            if should_use_vision:
                full_text = pdf_image_to_text_with_gemini(file_path)
            else:
                doc = fitz.open(file_path)
                for page in doc: full_text += page.get_text()

            # GÃ¼venlik AÄŸÄ±
            if not full_text.strip():
                 doc = fitz.open(file_path)
                 for page in doc: full_text += page.get_text()

            header_text = full_text[:300].replace("\n", " ").strip() if full_text else "BaÅŸlÄ±ksÄ±z"
            unified_doc = Document(
                page_content=f"BELGE KÄ°MLÄ°ÄÄ°: {header_text}\nKAYNAK DOSYA: {uploaded_file.name}\n---\n{full_text}",
                metadata={"source": uploaded_file.name}
            )
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500,     
                chunk_overlap=300,    
                
                separators=[
                    "\nMADDE",        # Ã–nce Maddelere gÃ¶re bÃ¶lmeye Ã§alÄ±ÅŸsÄ±n (En ideali)
                    "\nGEÃ‡Ä°CÄ° MADDE", # GeÃ§ici maddeleri de yakalayalÄ±m
                    "\n###",          # BaÅŸlÄ±klar
                    "\n\n",           # Paragraflar
                    "\n",             # SatÄ±rlar
                    ". ",             # CÃ¼mleler
                    " ",              # Kelimeler
                    ""                # Harfler (son Ã§are)
                ]
            )
            split_docs = text_splitter.split_documents([unified_doc])
            
            # Belgeleri ana listeye ekle (Burada uyumaya gerek yok)
            all_documents.extend(split_docs)
            
            if os.path.exists(file_path): os.remove(file_path)
            
            # Supabase iÅŸlemleri...
            try:
                uploaded_file.seek(0)
                file_bytes = uploaded_file.read()
                supabase.storage.from_("belgeler").upload(
                    path=uploaded_file.name, file=file_bytes,
                    file_options={"content-type": "application/pdf", "upsert": "true"}
                )
                supabase.table("dokumanlar").delete().eq("dosya_adi", uploaded_file.name).execute()
                supabase.table("dokumanlar").insert({"dosya_adi": uploaded_file.name}).execute()
            except: pass
            
        except Exception as e:
            st.error(f"Hata ({uploaded_file.name}): {e}")

    if all_documents:
        try:
            st.info(f"ğŸš€ Toplam {len(all_documents)} parÃ§a Google sunucularÄ±na parÃ§a parÃ§a iÅŸleniyor...")
            
            # 1. Ã–nce Modeli ve VektÃ¶r Store'u HazÄ±rla (BoÅŸ Olarak)
            embedding_model = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001",
                google_api_key=st.secrets["GOOGLE_API_KEY"]
            )
            
            # Pinecone baÄŸlantÄ±sÄ±nÄ± kur
            vector_store = PineconeVectorStore(
                index_name="mevzuat-asistani",
                embedding=embedding_model,
                pinecone_api_key=st.secrets["PINECONE_API_KEY"]
            )
            
            # 2. BATCH UPLOAD (VAGON SÄ°STEMÄ°) 
            # 100 parÃ§ayÄ± aynÄ± anda atmak yerine 10'ar 10'ar atÄ±p dinleniyoruz.
            batch_size = 10
            total_batches = len(all_documents) // batch_size + 1
            
            progress_bar = st.progress(0)
            
            for i in range(0, len(all_documents), batch_size):
                # 10 parÃ§alÄ±k vagonu al
                batch = all_documents[i : i + batch_size]
                
                if batch:
                    # Vagonu Pinecone'a gÃ¶nder
                    vector_store.add_documents(batch)
                    
                    # Ä°lerleme Ã§ubuÄŸunu gÃ¼ncelle
                    current_progress = min((i + batch_size) / len(all_documents), 1.0)
                    progress_bar.progress(current_progress)
                    
                    # Google KotasÄ± Ä°Ã§in Fren: Her vagondan sonra 2 saniye bekle
                    time.sleep(2)
            
            st.success("âœ… TÃ¼m belgeler baÅŸarÄ±yla vektÃ¶rleÅŸtirildi!")
            return vector_store
            
        except Exception as e:
            st.error(f"Pinecone/Embedding HatasÄ±: {str(e)}")
            return None
    
    return None

# --- DÄ°ÄERLERÄ° AYNI ---
def delete_document_cloud(file_name):
    # 1. Pinecone TemizliÄŸi (Hata Verirse YutacaÄŸÄ±z)
    try:
        pinecone_api_key = st.secrets["PINECONE_API_KEY"]
        index_name = "mevzuat-asistani"
        pc = Pinecone(api_key=pinecone_api_key)
        index = pc.Index(index_name)
        
        # Pinecone'dan silmeyi dene
        index.delete(filter={"source": file_name})
        
    except Exception as e:
        # Hata verirse (404 vs.) konsola yaz ama iÅŸlemi DURDURMA.
        # Ã‡Ã¼nkÃ¼ amaÃ§ zaten dosyadan kurtulmak.
        print(f"Pinecone silme uyarÄ±sÄ± (Ã–nemsiz): {e}")

    # 2. Supabase ve Storage TemizliÄŸi (AsÄ±l Kritik KÄ±sÄ±m)
    try:
        supabase = create_client(st.secrets["SUPABASE_URL"], st.secrets["SUPABASE_KEY"])
        
        # VeritabanÄ± kaydÄ±nÄ± sil
        supabase.table("dokumanlar").delete().eq("dosya_adi", file_name).execute()
        
        # DosyanÄ±n kendisini storage'dan sil
        supabase.storage.from_("belgeler").remove([file_name])
        
        return True, f"{file_name} baÅŸarÄ±yla temizlendi."
        
    except Exception as e: 
        return False, f"Supabase silme hatasÄ±: {str(e)}"

def connect_to_existing_index():
    try:
        embedding_model = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=st.secrets["GOOGLE_API_KEY"]
        )
        vector_store = PineconeVectorStore.from_existing_index(
            index_name="mevzuat-asistani",
            embedding=embedding_model
        )
        return vector_store
    except Exception as e: return None