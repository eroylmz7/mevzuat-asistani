# -----------------------------------------------------------------------------
# 1. BULUT VERÄ°TABANI YAMASI (EN ÃœSTTE OLMALI)
# -----------------------------------------------------------------------------
import sys
import os

try:
    __import__('pysqlite3')
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# -----------------------------------------------------------------------------
# KÃœTÃœPHANELER
# -----------------------------------------------------------------------------
import streamlit as st
import shutil
import time
import json
import datetime
from dotenv import load_dotenv

# RAG ve LangChain BileÅŸenleri (KARARLI SÃœRÃœM AYARLARI)
from langchain_community.vectorstores import Chroma
# Yeni "langchain_huggingface" yerine eski "community" iÃ§inden Ã§aÄŸÄ±rÄ±yoruz:
from langchain_community.embeddings import HuggingFaceEmbeddings 
from langchain_google_genai import ChatGoogleGenerativeAI
# Prompt ve Chain'leri ana paketten Ã§aÄŸÄ±rÄ±yoruz (0.1.20 sÃ¼rÃ¼mÃ¼ bunu destekler):
from langchain.prompts import PromptTemplate  
from langchain.chains import ConversationalRetrievalChain 

# Kendi fonksiyonlarÄ±mÄ±z
from data_ingestion import load_and_process_pdfs

# -----------------------------------------------------------------------------
# AYARLAR VE SABÄ°TLER
# -----------------------------------------------------------------------------
load_dotenv()

st.set_page_config(
    page_title="KampÃ¼s AsistanÄ±",
    page_icon="ğŸ“",
    layout="wide"
)

PERSIST_DIRECTORY = "./chroma_db_store"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# -----------------------------------------------------------------------------
# FONKSÄ°YONLAR
# -----------------------------------------------------------------------------

@st.cache_resource
def get_vector_db():
    # Model yÃ¼kleme
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    # 1. YÃ–NTEM: Mevcut veritabanÄ± kontrolÃ¼
    if os.path.exists(PERSIST_DIRECTORY):
        try:
            print("ğŸ’¾ Mevcut veritabanÄ± kontrol ediliyor...")
            vectordb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding)
            if vectordb._collection.count() > 0:
                print("âœ… VeritabanÄ± saÄŸlam.")
                return vectordb
        except Exception as e:
            print(f"âš ï¸ Hata: {e}")

    # 2. YÃ–NTEM: Otomatik OnarÄ±m (Auto-Healing)
    print("ğŸ”„ VeritabanÄ± sÄ±fÄ±rdan kuruluyor...")
    if os.path.exists("./veriler") and os.listdir("./veriler"):
        try:
            with st.spinner("Sistem hazÄ±rlanÄ±yor (Bu iÅŸlem bir kez yapÄ±lÄ±r)..."):
                chunks = load_and_process_pdfs()
                if chunks:
                    vectordb = Chroma.from_documents(chunks, embedding, persist_directory=PERSIST_DIRECTORY)
                    print("âœ… Kurulum tamamlandÄ±!")
                    return vectordb
        except Exception as e:
            st.error(f"âŒ Kurulum hatasÄ±: {e}")
            return None
    return None

def get_llm_chain(vectordb):
    # Gemini AyarlarÄ±
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.3)
    
    custom_template = """
    Sen Ã¼niversite mevzuatlarÄ± konusunda uzman bir asistansÄ±n.
    Kurallar:
    1. SADECE verilen baÄŸlamÄ± kullan.
    2. Tarih ve gÃ¼n hesaplamalarÄ± iÃ§in kendi bilgini kullan.
    3. Bilgi yoksa "Bilmiyorum" de.
    
    Sohbet GeÃ§miÅŸi:
    {chat_history}
    
    BaÄŸlam:
    {context}
    
    Soru: {question}
    Cevap:
    """
    
    PROMPT = PromptTemplate(template=custom_template, input_variables=["chat_history", "context", "question"])
    
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": PROMPT},
        verbose=False
    )
    return qa_chain

# -----------------------------------------------------------------------------
# ARAYÃœZ
# -----------------------------------------------------------------------------

if "logged_in" not in st.session_state:
    st.session_state.logged_in = False
if "username" not in st.session_state:
    st.session_state.username = ""
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

with st.sidebar:
    st.header("âš™ï¸ Panel")
    users = {}
    try:
        if os.path.exists("users.json"):
            with open("users.json", "r") as f:
                users = json.load(f)
    except: pass

    if not st.session_state.logged_in:
        u = st.text_input("KullanÄ±cÄ± AdÄ±")
        p = st.text_input("Åifre", type="password")
        if st.button("GiriÅŸ Yap"):
            if u in users and users[u]["password"] == p:
                st.session_state.logged_in = True
                st.session_state.username = u
                st.session_state.role = users[u]["role"]
                st.rerun()
            else:
                st.error("HatalÄ± giriÅŸ!")
    else:
        st.info(f"KullanÄ±cÄ±: {st.session_state.username}")
        if st.session_state.get("role") == "admin":
            st.divider()
            files = st.file_uploader("PDF YÃ¼kle", type=["pdf"], accept_multiple_files=True)
            if st.button("GÃ¼ncelle"):
                if files:
                    if not os.path.exists("./veriler"): os.makedirs("./veriler")
                    for f in files:
                        with open(os.path.join("./veriler", f.name), "wb") as w:
                            w.write(f.getbuffer())
                    shutil.rmtree(PERSIST_DIRECTORY, ignore_errors=True)
                    st.rerun()
        if st.button("Ã‡Ä±kÄ±ÅŸ"):
            st.session_state.logged_in = False
            st.rerun()

st.title("ğŸ“ Mevzuat AsistanÄ±")

if st.session_state.logged_in:
    vectordb = get_vector_db()
    if vectordb:
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])

        if prompt := st.chat_input("Sorunuzu yazÄ±n..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"): st.markdown(prompt)

            with st.chat_message("assistant"):
                placeholder = st.empty()
                placeholder.markdown("âš¡ *DÃ¼ÅŸÃ¼nÃ¼yor...*")
                try:
                    chain = get_llm_chain(vectordb)
                    res = chain({"question": prompt, "chat_history": st.session_state.chat_history})
                    answer = res['answer']
                    
                    # KaynaklarÄ± formatla
                    sources = []
                    seen = set()
                    for doc in res['source_documents']:
                        name = os.path.basename(doc.metadata.get('source', 'Belge'))
                        page = doc.metadata.get('page', 0) + 1
                        key = f"{name} (S.{page})"
                        if key not in seen:
                            sources.append(key)
                            seen.add(key)
                    
                    final = f"{answer}\n\nğŸ“š **Kaynaklar:**\n" + "\n".join([f"- {s}" for s in sources])
                    
                    # Streaming Efekti
                    def stream():
                        for word in final.split(" "):
                            yield word + " "
                            time.sleep(0.02)
                    placeholder.write_stream(stream)
                    
                    st.session_state.messages.append({"role": "assistant", "content": final})
                    st.session_state.chat_history.append((prompt, answer))
                except Exception as e:
                    placeholder.error(f"Hata: {e}")
    else:
        st.error("VeritabanÄ± ÅŸu an hazÄ±r deÄŸil. YÃ¶netici PDF yÃ¼klememiÅŸ olabilir.")
else:
    st.info("LÃ¼tfen giriÅŸ yapÄ±nÄ±z.")

# Ä°ndirme Butonu
if st.session_state.messages:
    st.markdown("---")
    txt = "\n\n".join([f"[{m['role'].upper()}]: {m['content']}" for m in st.session_state.messages])
    st.download_button("Sohbeti Ä°ndir", txt, "chat.txt")